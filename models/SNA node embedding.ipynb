{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48b4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import networkx as nx\n",
    "import networkx.algorithms as alg\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import RelGraphConv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from dgl.nn import GraphConv, AvgPooling\n",
    "from dgl.nn.pytorch import Sequential\n",
    "\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import pickle\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3821a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = 'nx_url_dataset.pickle'\n",
    "with open(pickle_name, 'rb') as f:\n",
    "    nx_train_set, nx_test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c825f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "e2id = {'title':0, \n",
    "        'link':1,\n",
    "        'reversed_title':2,\n",
    "        'reversed_link':3,\n",
    "        'domain':4\n",
    "       }\n",
    "\n",
    "def add_encoding(dataset):\n",
    "    for j in range(len(dataset)):\n",
    "        g = dataset[j][0]\n",
    "        \n",
    "        in_degree = nx.algorithms.in_degree_centrality(g)\n",
    "        out_degree = nx.algorithms.out_degree_centrality(g)\n",
    "        closeness = nx.algorithms.closeness_centrality(g)\n",
    "        clustering = nx.algorithms.clustering(g)\n",
    "        reversesed_graph = g.reverse(copy=True)\n",
    "        eig = nx.algorithms.eigenvector_centrality(reversesed_graph, max_iter=10000)\n",
    "\n",
    "        feats = np.asarray([list(in_degree.values()),\n",
    "                            list(out_degree.values()),\n",
    "                            list(closeness.values()),\n",
    "                            list(eig.values()),\n",
    "                            list(clustering.values())\n",
    "                           ])\n",
    "        mean = feats.mean(1)\n",
    "        std = feats.std(1)\n",
    "\n",
    "        for e in g.edges:\n",
    "#             print(g.edges[e[0], e[1]]['link_type'])\n",
    "            if g.edges[e[0], e[1]]['link_type'] == 'title':\n",
    "                g.edges[e[0], e[1]]['type'] = e2id['title']\n",
    "                g.add_edge(e[1], e[0])\n",
    "                g.edges[e[1], e[0]]['type'] = e2id['reversed_title']\n",
    "            \n",
    "                \n",
    "            if g.edges[e[0], e[1]]['link_type'] == 'link':\n",
    "                g.edges[e[0], e[1]]['type'] = e2id['link']\n",
    "                g.add_edge(e[1], e[0])\n",
    "                g.edges[e[1], e[0]]['type'] = e2id['reversed_link']\n",
    "                \n",
    "        node_domains = dict()\n",
    "        for i in range(len(g.nodes)):\n",
    "#             g.nodes[i]['h'] = torch.tensor([])\n",
    "            feats = np.asarray([in_degree[i], out_degree[i], closeness[i], eig[i], clustering[i]])\n",
    "            feats -= mean\n",
    "            feats /= (std + 1e-6)\n",
    "            g.nodes[i]['h'] = torch.tensor(feats)\n",
    "#             g.nodes[i]['h'][g.nodes[i]['level']] = 1.\n",
    "            if g.nodes[i]['domain'] not in node_domains:\n",
    "                node_domains[g.nodes[i]['domain']] = [i]\n",
    "            else:\n",
    "                node_domains[g.nodes[i]['domain']].append(i)\n",
    "            \n",
    "        for domain in node_domains:\n",
    "            if len(node_domains[domain]) >1:\n",
    "                for d1 in node_domains[domain]:\n",
    "                    for d2 in node_domains[domain]:\n",
    "                        if d1 == d2:\n",
    "                            continue\n",
    "                        g.add_edge(d1, d2)\n",
    "                        g.edges[d1, d2]['type'] = e2id['domain']\n",
    "                        \n",
    "    return dataset\n",
    "train_set = add_encoding(nx_train_set)\n",
    "test_set = add_encoding(nx_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbf33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = [(dgl.from_networkx(g, node_attrs=[\"h\"], edge_attrs=['type']), torch.tensor([l]).float()) for g, l in train_set]\n",
    "train_labels = [l for _, l in train_set]\n",
    "test_graphs = [(dgl.from_networkx(g, node_attrs=['h'], edge_attrs=['type']), torch.tensor([l]).float()) for g, l in test_set]\n",
    "test_labels = [l for _, l in test_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab938006",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [g for g,_ in train_graphs]\n",
    "g = dgl.batch(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1ef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, dropout, num_edge_types):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.in_layer = RelGraphConv(in_features, hidden_features, num_edge_types, activation=nn.ReLU())\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.out_layer = RelGraphConv(hidden_features, hidden_features, num_edge_types, activation=nn.ReLU())\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.global_mean_pool = dgl.nn.pytorch.glob.AvgPooling()\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, g, in_feat, e_types):\n",
    "        h = self.in_layer(g, in_feat, e_types)\n",
    "        h = self.dropout1(h)\n",
    "        h = self.out_layer(g, h, e_types)\n",
    "        h = self.dropout2(h)\n",
    "        h = self.global_mean_pool(g, h)\n",
    "        h = self.fc(h)\n",
    "        \n",
    "        return self.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3bd82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "        pred = torch.round(model(batched_graph, batched_graph.ndata['h'].float(), batched_graph.edata['type']))\n",
    "        num_correct += (pred==labels).sum().float().item()\n",
    "        num_tests += len(labels)\n",
    "    return num_correct / num_tests\n",
    "\n",
    "def train(model, train_loader, optimizer):\n",
    "    Loss = nn.BCELoss()\n",
    "    \n",
    "    model.train()\n",
    "    ret_loss = 0\n",
    "    n_graphs = 0\n",
    "    for data, target in train_loader:\n",
    "        output = model(data, data.ndata['h'].float(), data.edata['type'])\n",
    "        \n",
    "        loss = Loss(output, target)\n",
    "        ret_loss += loss.item()/len(target)\n",
    "        n_graphs += len(target)     \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "#     return ret_loss/n_graphs\n",
    "\n",
    "def val_loss(model, loader):\n",
    "    Loss = nn.BCELoss()\n",
    "    model.eval()\n",
    "    ret_loss = 0\n",
    "    n_graphs = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data, data.ndata['h'].float(), data.edata['type'])\n",
    "            loss = Loss(output, target)\n",
    "\n",
    "            return loss.item()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024633a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.in_layer.linear_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7b90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "splits=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "split_idxs = splits.split(np.arange(len(train_graphs)), train_labels)\n",
    "\n",
    "num_epochs=100\n",
    "from itertools import product\n",
    "hidden_size = [4, 16, 32]\n",
    "dropout = [0., 0.5, 0.7]\n",
    "\n",
    "lr = [0.05, 0.01, 0.005]\n",
    "\n",
    "tg = dgl.batch([g for g,_ in train_graphs])\n",
    "\n",
    "model_params = list(product(hidden_size, dropout, lr))\n",
    "\n",
    "in_feature_size = train_graphs[0][0].ndata['h'].shape[1]\n",
    "num_edge_types = tg.edata['type'].max().item() +1\n",
    "print(num_edge_types)\n",
    "\n",
    "# model_params = [[4, 0., 0.1]]\n",
    "# print(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ec66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for l in splits.split(np.arange(len(train_graphs)), train_labels):\n",
    "#     print(train_labels[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d22ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0/27\n",
      "Model parameters: hidden_size 4, dropout 0.0, optimizer lr 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1/27\n",
      "Model parameters: hidden_size 4, dropout 0.0, optimizer lr 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 2/27\n",
      "Model parameters: hidden_size 4, dropout 0.0, optimizer lr 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 3/27\n",
      "Model parameters: hidden_size 4, dropout 0.5, optimizer lr 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 4/27\n",
      "Model parameters: hidden_size 4, dropout 0.5, optimizer lr 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 5/27\n",
      "Model parameters: hidden_size 4, dropout 0.5, optimizer lr 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 6/27\n",
      "Model parameters: hidden_size 4, dropout 0.7, optimizer lr 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 7/27\n",
      "Model parameters: hidden_size 4, dropout 0.7, optimizer lr 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 8/27\n",
      "Model parameters: hidden_size 4, dropout 0.7, optimizer lr 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 9/27\n",
      "Model parameters: hidden_size 16, dropout 0.0, optimizer lr 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 10/27\n",
      "Model parameters: hidden_size 16, dropout 0.0, optimizer lr 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 11/27\n",
      "Model parameters: hidden_size 16, dropout 0.0, optimizer lr 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 12/27\n",
      "Model parameters: hidden_size 16, dropout 0.5, optimizer lr 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 13/27\n",
      "Model parameters: hidden_size 16, dropout 0.5, optimizer lr 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 14/27\n",
      "Model parameters: hidden_size 16, dropout 0.5, optimizer lr 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 15/27\n",
      "Model parameters: hidden_size 16, dropout 0.7, optimizer lr 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 16/27\n",
      "Model parameters: hidden_size 16, dropout 0.7, optimizer lr 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 17/27\n",
      "Model parameters: hidden_size 16, dropout 0.7, optimizer lr 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 18/27\n",
      "Model parameters: hidden_size 32, dropout 0.0, optimizer lr 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 19/27\n",
      "Model parameters: hidden_size 32, dropout 0.0, optimizer lr 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ondra\\anaconda3\\envs\\cp\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "best_test_acc = 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = []\n",
    "for i, (hidden_size, dropout, lr) in enumerate(model_params):\n",
    "    foldperf = {}\n",
    "    print(f\"model {i}/{len(model_params)}\")\n",
    "    print('Model parameters: hidden_size {}, dropout {}, optimizer lr {}'.format(*model_params[i]))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(train_graphs)), train_labels)):\n",
    "        model = RGCN(in_feature_size, hidden_size, dropout, num_edge_types)\n",
    "\n",
    "        lrs = []\n",
    "\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = GraphDataLoader(train_graphs, batch_size=1000, sampler=train_sampler)\n",
    "        test_loader = GraphDataLoader(train_graphs, batch_size=1000, sampler=test_sampler)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = train(model,train_loader, optimizer)\n",
    "            train_acc = evaluate(model, train_loader)\n",
    "            test_loss = val_loss(model,test_loader)\n",
    "            test_acc = evaluate(model, test_loader)\n",
    "#             if epoch%10==1:\n",
    "#                 print(\"{} Loss train/val:{:.3f} / {:.3f} Acc train/val {:.2f} / {:.2f} %\".format(epoch, train_loss,\n",
    "#                                                                                               test_loss,\n",
    "#                                                                                               100*train_acc,\n",
    "#                                                                                               100*test_acc))\n",
    "\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_acc'].append(test_acc)\n",
    "        foldperf[f'fold{fold+1}'] = history  \n",
    "    results.append(foldperf)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410baea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, foldperf in enumerate(results):\n",
    "    train_loss = np.vstack([foldperf[fold]['train_loss'] for fold in foldperf])\n",
    "    test_loss = np.vstack([foldperf[fold]['test_loss'] for fold in foldperf])\n",
    "    train_acc = np.vstack([foldperf[fold]['train_acc'] for fold in foldperf])\n",
    "    test_acc = np.vstack([foldperf[fold]['test_acc'] for fold in foldperf])\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(9, 9))\n",
    "    fig.suptitle(f'Model {i}, hidden_size {model_params[i][0]}, dropout {model_params[i][1]}, lr {model_params[i][2]}')\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.title.set_text('Training loss')\n",
    "    ax1.fill_between(range(num_epochs), train_loss.min(0), train_loss.max(0), alpha=0.3, label=\"train loss <min; max>\")\n",
    "    ax1.fill_between(range(num_epochs), test_loss.min(0), test_loss.max(0), alpha=0.3, label=\"test loss <min; max>\")\n",
    "\n",
    "    ax1.plot(range(num_epochs), train_loss.mean(0), label='train loss mean')\n",
    "    ax1.plot(range(num_epochs), test_loss.mean(0), label='test loss mean')\n",
    "    ax1.legend()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax2.title.set_text(\"Training loss with cross validation\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.fill_between(range(num_epochs), train_acc.min(0), train_acc.max(0), alpha=0.2, label=\"train acc <min; max>\")\n",
    "    ax2.fill_between(range(num_epochs), test_acc.min(0), test_acc.max(0), alpha=0.2, label=\"test acc <min; max>\")\n",
    "\n",
    "    ax2.plot(range(num_epochs), train_acc.mean(0))\n",
    "\n",
    "    ax2.plot(range(num_epochs), test_acc.mean(0))\n",
    "    ax2.legend()\n",
    "\n",
    "    \n",
    "    \n",
    "    ax3.title.set_text('Train loss per fold')\n",
    "    for l in train_loss:\n",
    "        ax3.plot(l)\n",
    "#     for l in train_loss:\n",
    "#         ax3.plot(l, 'b-', alpha=0.5)\n",
    "#     for l in test_loss:\n",
    "#         ax3.plot(l, 'r-', alpha=0.5)\n",
    "#     ax3.plot(np.convolve(train_loss.mean(0), np.ones(20)/20, mode='valid'), label='train loss')\n",
    "#     ax3.plot(np.convolve(test_loss.mean(0), np.ones(20)/20, mode='valid'), label='test loss')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss')\n",
    "\n",
    "    ax3.legend()\n",
    "    \n",
    "    ax4.title.set_text('Validation loss per fold')\n",
    "    for l in test_loss:\n",
    "        ax4.plot(l)\n",
    "#     ax3.plot(np.convolve(train_loss.mean(0), np.ones(20)/20, mode='valid'), label='train loss')\n",
    "#     ax3.plot(np.convolve(test_loss.mean(0), np.ones(20)/20, mode='valid'), label='test loss')\n",
    "    ax4.set_ylabel('Accuracy')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ae329",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for i, foldperf in enumerate(results):\n",
    "    accs.append(np.vstack([foldperf[fold]['test_acc'][-1] for fold in foldperf]).mean(0))\n",
    "best_params = model_params[np.asarray(accs).argmax()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, dropout, lr = best_params\n",
    "model = RGCN(hidden_size, dropout, num_edge_types)\n",
    "\n",
    "train_loader = GraphDataLoader(train_graphs, batch_size=1000)\n",
    "# test_loader = GraphDataLoader(train_graphs, batch_size=1000)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model,train_loader,optimizer)\n",
    "    acc = evaluate(model, train_loader)\n",
    "\n",
    "\n",
    "\n",
    "    train_loss.append(loss)\n",
    "    train_acc.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ca974",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10, 5))\n",
    "plt.suptitle('Training loss and acc for the best model')\n",
    "ax1.set_xlabel('eppch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.plot(train_loss)\n",
    "\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('Acc [%]')\n",
    "ax2.plot(train_acc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa41070",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### test_loader = GraphDataLoader(test_graphs, batch_size=1000)\n",
    "\n",
    "print('best params: hidden size {}, dropout {}, Adam learning rate{}'.format(*best_params))\n",
    "acc = evaluate(model, test_loader)\n",
    "print(f'Model test acc is {acc*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
